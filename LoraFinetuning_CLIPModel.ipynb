{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kanfe5AjLCBU"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "#parent_dir = \"/content/drive/MyDrive/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygVjQSuWAiB0"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcCWhHaKdAhw"
   },
   "source": [
    "# Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1ZTwTFC_ekL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.datasets import Caltech101\n",
    "from torch.utils.data import random_split\n",
    "import time\n",
    "import os\n",
    "\n",
    "torch.backends.cudnn.enabled = True  # Enabled by default\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5InTLhedFGo"
   },
   "source": [
    "# Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkJemy02_o1e"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "  def __init__(self, width, max_seq_length):\n",
    "    super().__init__()\n",
    "\n",
    "    # Creating positional encoding\n",
    "    pe = torch.zeros(max_seq_length, width)\n",
    "\n",
    "    for pos in range(max_seq_length):\n",
    "      for i in range(width):\n",
    "        if i % 2 == 0:\n",
    "          pe[pos][i] = np.sin(pos/(10000 ** (i/width)))\n",
    "        else:\n",
    "          pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/width)))\n",
    "\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Add positional encoding to embeddings\n",
    "    x = x + self.pe\n",
    "    return x\n",
    "\n",
    "\n",
    "pe = PositionalEmbedding(width = 8, max_seq_length=32)\n",
    "print(f\"p.pe.shape:{pe.pe.shape}\")\n",
    "x = torch.randn(1,32,8)\n",
    "print(f\"x.shape:{x.shape}\")\n",
    "res = pe(x)\n",
    "print(f\"res:{res.shape}\")\n",
    "\n",
    "trainable_params = sum(p.numel() for p in pe.parameters() if p.requires_grad)\n",
    "print(f\"Starting training with: {trainable_params} parameters\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BShVHkuAdIV0"
   },
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHKmUq7V_qhH"
   },
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, width, head_size, lora_rank):\n",
    "    super().__init__()\n",
    "    self.head_size = head_size\n",
    "\n",
    "    self.query = nn.Linear(width, head_size)\n",
    "    self.key = nn.Linear(width, head_size)\n",
    "    self.value = nn.Linear(width, head_size)\n",
    "\n",
    "    self.qA = nn.Linear(width, lora_rank)\n",
    "    self.qB = nn.Linear(lora_rank, head_size)\n",
    "\n",
    "    #self.kA = nn.Linear(width, lora_rank)\n",
    "    #self.kB = nn.Linear(lora_rank, head_size)\n",
    "\n",
    "    #self.vA = nn.Linear(width, lora_rank)\n",
    "    #self.vB = nn.Linear(lora_rank, head_size)\n",
    "\n",
    "    #nn.init.constant_(self.qB.weight, 0)\n",
    "    #nn.init.constant_(self.qB.bias, 0)\n",
    "\n",
    "    #nn.init.constant_(self.kB.weight, 0)\n",
    "    #nn.init.constant_(self.kB.bias, 0)\n",
    "\n",
    "    #nn.init.constant_(self.vB.weight, 0)\n",
    "    #nn.init.constant_(self.vB.bias, 0)\n",
    "\n",
    "    self.set_lora_mode(False)\n",
    "\n",
    "\n",
    "  def set_lora_mode(self, mode):\n",
    "\n",
    "    self.query.requires_grad_(not mode)\n",
    "    self.key.requires_grad_(not mode)\n",
    "    self.value.requires_grad_(not mode)\n",
    "\n",
    "    self.qA.requires_grad_(mode)\n",
    "    self.qB.requires_grad_(mode)\n",
    "\n",
    "    #self.kA.requires_grad_(mode)\n",
    "    #self.kB.requires_grad_(mode)\n",
    "\n",
    "    #self.vA.requires_grad_(mode)\n",
    "    #self.vB.requires_grad_(mode)\n",
    "\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    # Obtaining Queries, Keys, and Values\n",
    "    Q = self.query(x) + self.qB(self.qA(x))\n",
    "    K = self.key(x)# + self.kB(self.kA(x))\n",
    "    V = self.value(x)# + self.vB(self.vA(x))\n",
    "\n",
    "    # Dot Product of Queries and Keys\n",
    "    attention = Q @ K.transpose(-2,-1)\n",
    "    #print(f\"attention matrix: {attention.shape}\")\n",
    "\n",
    "    # Scaling\n",
    "    attention = attention / (self.head_size ** 0.5)\n",
    "\n",
    "    # Applying Attention Mask\n",
    "    if mask is not None:\n",
    "        attention = attention.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "    attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "    attention = attention @ V\n",
    "    #print(f\"attention vectors: {attention.shape}\")\n",
    "\n",
    "    return attention\n",
    "ah = AttentionHead(width = 8, head_size = 4, lora_rank = 1)\n",
    "print(f\"query.weight:{ah.query.weight} {ah.query.in_features}:{ah.query.out_features}\")\n",
    "print(f\"qA.weight:{ah.qA.weight} {ah.qA.in_features}:{ah.qA.out_features}\")\n",
    "print(f\"qB.weight:{ah.qB.weight} {ah.qB.in_features}:{ah.qB.out_features}\")\n",
    "print()\n",
    "print(f\"key.weight:{ah.key.weight} {ah.key.in_features}:{ah.key.out_features}\")\n",
    "#print(f\"kA.weight:{ah.kA.weight} {ah.kA.in_features}:{ah.kA.out_features}\")\n",
    "#print(f\"kB.weight:{ah.kB.weight} {ah.kB.in_features}:{ah.kB.out_features}\")\n",
    "print()\n",
    "print(f\"value.weight:{ah.value.weight} {ah.value.in_features}:{ah.value.out_features}\")\n",
    "#print(f\"vA.weight:{ah.vA.weight} {ah.vA.in_features}:{ah.vA.out_features}\")\n",
    "#print(f\"vB.weight:{ah.vB.weight} {ah.vB.in_features}:{ah.vB.out_features}\")\n",
    "\n",
    "#32 tokens 8 features per token\n",
    "short_input = torch.randn(1,32,8)\n",
    "print(f\"short_input.shape:{short_input.shape}\")\n",
    "short_output = ah(short_input)\n",
    "print(f\"short_output.shape:{short_output.shape}\")\n",
    "print()\n",
    "\n",
    "#128 tokens 8 features per token\n",
    "long_input = torch.randn(1,128,8)\n",
    "print(f\"long_input.shape:{long_input.shape}\")\n",
    "long_output = ah(long_input)\n",
    "print(f\"long_output.shape:{long_output.shape}\")\n",
    "\"\"\"\n",
    "illegal_input = torch.randn(1,128,16)\n",
    "print(f\"illegal_input.shape:{illegal_input.shape}\")\n",
    "illegal_output = AttentionHead(illegal_input)\n",
    "print(f\"illegal_output.shape:{illegal_output.shape}\")\n",
    "\"\"\"\n",
    "trainable_params = sum(p.numel() for p in ah.parameters() if p.requires_grad)\n",
    "print(f\"Initializing with: {trainable_params} parameters\")\n",
    "\n",
    "ah.set_lora_mode(False)\n",
    "trainable_params = sum(p.numel() for p in ah.parameters() if p.requires_grad)\n",
    "print(f\"Training with: {trainable_params} parameters\")\n",
    "\n",
    "\n",
    "ah.set_lora_mode(True)\n",
    "trainable_params = sum(p.numel() for p in ah.parameters() if p.requires_grad)\n",
    "print(f\"Finetuning with: {trainable_params} parameters\")\n",
    "\n",
    "\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLdbE5aLG4fH"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, width, n_heads, lora_rank):\n",
    "    super().__init__()\n",
    "    self.head_size = width // n_heads\n",
    "\n",
    "    self.W_o = nn.Linear(width, width)\n",
    "\n",
    "    #self.oA = nn.Linear(width, lora_rank)\n",
    "    #self.oB = nn.Linear(lora_rank, width)\n",
    "\n",
    "    #nn.init.constant_(self.oB.weight, 0)\n",
    "    #nn.init.constant_(self.oB.bias, 0)\n",
    "\n",
    "    self.heads = nn.ModuleList([AttentionHead(width, self.head_size, lora_rank) for _ in range(n_heads)])\n",
    "    self.set_lora_mode(False)\n",
    "\n",
    "  def set_lora_mode(self, mode):\n",
    "    for head in self.heads:\n",
    "      head.set_lora_mode(mode)\n",
    "\n",
    "    self.W_o.requires_grad_(not mode)\n",
    "    #self.oA.requires_grad_(mode)\n",
    "    #self.oB.requires_grad_(mode)\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    # Combine attention heads\n",
    "    out = torch.cat([head(x, mask=mask) for head in self.heads], dim=-1)\n",
    "\n",
    "    out = self.W_o(out)# + self.oB(self.oA(out))\n",
    "\n",
    "    return out\n",
    "\n",
    "MHA = MultiHeadAttention(width = 32, n_heads = 4, lora_rank = 1)\n",
    "print(f\"W_o.weight:{MHA.W_o.weight} {MHA.W_o.in_features}:{MHA.W_o.out_features}\")\n",
    "#print(f\"oA.weight:{MHA.oA.weight} {MHA.oA.in_features}:{MHA.oA.out_features}\")\n",
    "#print(f\"oB.weight:{MHA.oB.weight} {MHA.oB.in_features}:{MHA.oB.out_features}\")\n",
    "print()\n",
    "\n",
    "#32 tokens 8 features per token\n",
    "short_input = torch.randn(1,36,32)\n",
    "print(f\"short_input.shape:{short_input.shape}\")\n",
    "short_output = MHA(short_input)\n",
    "print(f\"short_output.shape:{short_output.shape}\")\n",
    "print()\n",
    "\n",
    "#128 tokens 8 features per token\n",
    "long_input = torch.randn(1,128,32)\n",
    "print(f\"long_input.shape:{long_input.shape}\")\n",
    "long_output = MHA(long_input)\n",
    "print(f\"long_output.shape:{long_output.shape}\")\n",
    "print()\n",
    "\n",
    "\"\"\"\"\n",
    "illegal_input = torch.randn(1,128,16)\n",
    "print(f\"illegal_input.shape:{illegal_input.shape}\")\n",
    "illegal_output = MHA(illegal_input)\n",
    "print(f\"illegal_output.shape:{illegal_output.shape}\")\n",
    "\"\"\"\n",
    "trainable_params = sum(p.numel() for p in MHA.parameters() if p.requires_grad)\n",
    "print(f\"Initializing with: {trainable_params} parameters\")\n",
    "\n",
    "MHA.set_lora_mode(False)\n",
    "trainable_params = sum(p.numel() for p in MHA.parameters() if p.requires_grad)\n",
    "print(f\"Training with: {trainable_params} parameters\")\n",
    "\n",
    "\n",
    "MHA.set_lora_mode(True)\n",
    "trainable_params = sum(p.numel() for p in MHA.parameters() if p.requires_grad)\n",
    "print(f\"Finetuning with: {trainable_params} parameters\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wYeHJF2dPH_"
   },
   "source": [
    "# Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlLXEFKY_sI3"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, width, n_heads, lora_rank, r_mlp=4):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Sub-Layer 1 Normalization\n",
    "        self.ln1 = nn.LayerNorm(width)\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        self.mha = MultiHeadAttention(width, n_heads, lora_rank)\n",
    "\n",
    "        # Sub-Layer 2 Normalization\n",
    "        self.ln2 = nn.LayerNorm(width)\n",
    "\n",
    "        # Multilayer Perception\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.width, self.width*r_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.width*r_mlp, self.width)\n",
    "        )\n",
    "        self.set_lora_mode(False)\n",
    "    def set_lora_mode(self, mode):\n",
    "      self.mha.set_lora_mode(mode)\n",
    "      self.mlp.requires_grad_(not mode)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Residual Connection After Sub-Layer 1\n",
    "        x = x + self.mha(self.ln1(x), mask=mask)\n",
    "\n",
    "        # Residual Connection After Sub-Layer 2\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "TE = TransformerEncoder(width = 32, n_heads = 4, lora_rank=1)\n",
    "#32 tokens 8 features per token\n",
    "short_input = torch.randn(1,36,32)\n",
    "print(f\"short_input.shape:{short_input.shape}\")\n",
    "short_output = TE(short_input)\n",
    "print(f\"short_output.shape:{short_output.shape}\")\n",
    "print()\n",
    "\n",
    "#128 tokens 8 features per token\n",
    "long_input = torch.randn(1,128,32)\n",
    "print(f\"long_input.shape:{long_input.shape}\")\n",
    "long_output = TE(long_input)\n",
    "print(f\"long_output.shape:{long_output.shape}\")\n",
    "print()\n",
    "\n",
    "\"\"\"\n",
    "illegal_input = torch.randn(1,128,16)\n",
    "print(f\"illegal_input.shape:{illegal_input.shape}\")\n",
    "illegal_output = MHA(illegal_input)\n",
    "print(f\"illegal_output.shape:{illegal_output.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "trainable_params = sum(p.numel() for p in TE.parameters() if p.requires_grad)\n",
    "print(f\"Initializing with: {trainable_params} parameters\")\n",
    "\n",
    "TE.set_lora_mode(False)\n",
    "trainable_params = sum(p.numel() for p in TE.parameters() if p.requires_grad)\n",
    "print(f\"Training with: {trainable_params} parameters\")\n",
    "\n",
    "\n",
    "TE.set_lora_mode(True)\n",
    "trainable_params = sum(p.numel() for p in TE.parameters() if p.requires_grad)\n",
    "print(f\"Finetuning with: {trainable_params} parameters\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxNu9T7fdRlD"
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oo4r9vqWdViQ"
   },
   "outputs": [],
   "source": [
    "def tokenizer(text, max_seq_length, encode=True, mask=None):\n",
    "    if encode:\n",
    "        out = chr(2) + text + chr(3) # Adding SOT and EOT tokens\n",
    "        out = out + \"\".join([chr(0) for _ in range(max_seq_length-len(out))]) # Adding Padding\n",
    "        out = torch.IntTensor(list(out.encode(\"utf-8\"))) # Encoding Text\n",
    "        mask = torch.ones(len(out.nonzero()))\n",
    "        mask = torch.cat((mask,torch.zeros(max_seq_length-len(mask)))).type(torch.IntTensor)\n",
    "    else:\n",
    "        out = [chr(x) for x in text[1:len(mask.nonzero())-1]]\n",
    "        out = \"\".join(out)\n",
    "        mask = None\n",
    "\n",
    "    return out, mask\n",
    "\"\"\"\n",
    "short_input = \"hello world\"\n",
    "print(f\"len short input: {len(short_input)}\")\n",
    "tokens, mask = tokenizer(\"hello world\", encode = True, mask=None, max_seq_length=32)\n",
    "print(tokens)\n",
    "print(mask)\n",
    "\n",
    "long_input = (\"0123456789\"*3)\n",
    "print(f\"len long input: {len(long_input)}\") #start and end tokens added\n",
    "tokens, mask = tokenizer(long_input, encode = True, mask=None, max_seq_length=32)\n",
    "print(tokens)\n",
    "print(mask)\n",
    "\n",
    "\n",
    "illegal_input = (\"0123456789\"*4)images, labels = data[\"image\"].to(device), data[\"caption\"].to(device)\n",
    "        image_features = model.image_encoder(images)\n",
    "        text_features = model.text_encoder(text, mask=mask)\n",
    "\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * (image_features @ text_features.T)).softmax(dim=-1)\n",
    "        _, indices = torch.max(similarity,1)\n",
    "        pred = torch.stack([tokenizer(test_set.captions[int(i)])[0] for i in indices]).to(device)\n",
    "        correct += int(sum(torch.sum((pred==labels),dim=1)//len(pred[0])))\n",
    "        total += len(labels)\n",
    "print(f\"len illegal input: {len(illegal_input)}\") #start and end tokens added\n",
    "tokens, mask = tokenizer(illegal_input, encode = True, mask=None, max_seq_length=32)\n",
    "print(tokens)\n",
    "print(mask)\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdBJQrIsdVxD"
   },
   "source": [
    "# Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHEAV-Vq5Df5"
   },
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, width, max_seq_length, n_heads, n_layers, emb_dim, lora_rank):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_seq_length = max_seq_length  # Maximum length of input sequence\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, width) # Embedding Table\n",
    "\n",
    "        self.positional_embedding = PositionalEmbedding(width, max_seq_length)\n",
    "\n",
    "        self.encoder = nn.ModuleList([TransformerEncoder(width, n_heads, lora_rank) for _ in range(n_layers)])\n",
    "\n",
    "        # learned proj of image to embed\n",
    "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
    "        self.set_lora_mode(False)\n",
    "\n",
    "    def set_lora_mode(self, mode):\n",
    "      self.encoder_embedding.requires_grad_(not mode)\n",
    "      self.positional_embedding.requires_grad_(not mode)\n",
    "      for encoder in self.encoder:\n",
    "        encoder.set_lora_mode(mode)\n",
    "      self.projection.requires_grad_(not mode)\n",
    "\n",
    "    def forward(self, text, mask):\n",
    "        # Text Embedding\n",
    "        x = self.encoder_embedding(text)\n",
    "\n",
    "        # Positional Embedding\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x, mask=mask)\n",
    "        #print(f\"x:{x.size()}\")\n",
    "        #print(f\"text: {text.size()}\")\n",
    "        #print(f\"mask:{mask.size()}\")\n",
    "        # Takes features from the EOT Embedding\n",
    "        x = x[torch.arange(text.shape[0]),torch.sub(torch.sum(mask[:,0],dim=1),1)]\n",
    "\n",
    "        # joint multimodal embedding\n",
    "        if self.projection is not None:\n",
    "            x = x @ self.projection\n",
    "\n",
    "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "TE = TextEncoder(vocab_size=256, width=32, max_seq_length=128, n_heads=4, n_layers=4, emb_dim=32, lora_rank=1)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in TE.parameters() if p.requires_grad)\n",
    "print(f\"Initializing with: {trainable_params} parameters\")\n",
    "\n",
    "TE.set_lora_mode(False)\n",
    "trainable_params = sum(p.numel() for p in TE.parameters() if p.requires_grad)\n",
    "print(f\"Training with: {trainable_params} parameters\")\n",
    "\n",
    "\n",
    "TE.set_lora_mode(True)\n",
    "trainable_params = sum(p.numel() for p in TE.parameters() if p.requires_grad)\n",
    "print(f\"Finetuning with: {trainable_params} parameters\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZzh6RMgdXt3"
   },
   "source": [
    "# Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1WvEoo4_uLi"
   },
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, width, img_size, patch_size, n_channels, n_layers, n_heads, emb_dim, lora_rank):\n",
    "        super().__init__()\n",
    "\n",
    "        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "        assert width % n_heads == 0, \"width must be divisible by n_heads\"\n",
    "\n",
    "        self.n_patches = (img_size[0] * img_size[1]) // (patch_size[0] * patch_size[1])\n",
    "\n",
    "        self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "        # Patch Embedding\n",
    "        self.linear_project = nn.Conv2d(n_channels, width, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        # Classification Token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, width))\n",
    "\n",
    "        self.positional_embedding = PositionalEmbedding(width,self.max_seq_length)\n",
    "\n",
    "        self.encoder = nn.ModuleList([TransformerEncoder(width,n_heads, lora_rank) for _ in range(n_layers)])\n",
    "\n",
    "        # learned proj of image to embed\n",
    "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
    "        self.set_lora_mode(False)\n",
    "\n",
    "\n",
    "    def set_lora_mode(self, mode):\n",
    "      self.linear_project.requires_grad_(not mode)\n",
    "      self.cls_token.requires_grad_(not mode)\n",
    "      for encoder in self.encoder:\n",
    "        encoder.set_lora_mode(mode)\n",
    "      self.projection.requires_grad_(not mode)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Patch Embedding\n",
    "        x = self.linear_project(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # Positional Embedding\n",
    "        x = torch.cat((self.cls_token.expand(x.size()[0], -1, -1),x), dim=1)\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x)\n",
    "\n",
    "        # Takes Class Tokens\n",
    "        x = x[:, 0, :]\n",
    "\n",
    "        # joint multimodal embedding\n",
    "        if self.projection is not None:\n",
    "            x = x @ self.projection\n",
    "\n",
    "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "\n",
    "        return x\n",
    "\n",
    "IE = TextEncoder(vocab_size=256, width=32, max_seq_length=128, n_heads=4, n_layers=4, emb_dim=32, lora_rank=1)\n",
    "\n",
    "\n",
    "trainable_params = sum(p.numel() for p in IE.parameters() if p.requires_grad)\n",
    "print(f\"Initializing with: {trainable_params} parameters\")\n",
    "\n",
    "IE.set_lora_mode(False)\n",
    "trainable_params = sum(p.numel() for p in IE.parameters() if p.requires_grad)\n",
    "print(f\"Training with: {trainable_params} parameters\")\n",
    "\n",
    "\n",
    "IE.set_lora_mode(True)\n",
    "trainable_params = sum(p.numel() for p in IE.parameters() if p.requires_grad)\n",
    "print(f\"Finetuning with: {trainable_params} parameters\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y07tBsSNdZmd"
   },
   "source": [
    "# CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6t-K-tnv_xwk"
   },
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers, lora_rank):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_encoder = ImageEncoder(vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, emb_dim, lora_rank)\n",
    "\n",
    "        self.text_encoder = TextEncoder(vocab_size, text_width, max_seq_length, text_heads, text_layers, emb_dim, lora_rank)\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"useing: {self.device}\")\n",
    "        self.set_lora_mode(False)\n",
    "\n",
    "    def set_lora_mode(self, mode):\n",
    "      self.image_encoder.set_lora_mode(mode)\n",
    "      self.text_encoder.set_lora_mode(mode)\n",
    "\n",
    "\n",
    "    def forward(self,image,text, mask=None):\n",
    "        I_e = self.image_encoder(image)\n",
    "        T_e = self.text_encoder(text, mask=mask)\n",
    "\n",
    "        # scaled pairwise cosine similarities [n, n]\n",
    "        logits = (I_e @ T_e.transpose(-2,-1)) * torch.exp(self.temperature)\n",
    "\n",
    "        # symmetric loss function\n",
    "        labels = torch.arange(logits.shape[0]).to(self.device)\n",
    "\n",
    "        loss_i = nn.functional.cross_entropy(logits.transpose(-2,-1), labels)\n",
    "        loss_t = nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        loss = (loss_i + loss_t) / 2\n",
    "\n",
    "        return loss\n",
    "\n",
    "C = CLIP(emb_dim=64, vit_width=128, img_size=(224,224), patch_size=(14,14), n_channels=1, vit_layers=4, vit_heads=4, vocab_size=256, text_width=128, max_seq_length=64, text_heads=5, text_layers=2, lora_rank=1)\n",
    "\n",
    "\n",
    "trainable_params = sum(p.numel() for p in C.parameters() if p.requires_grad)\n",
    "print(f\"Initializing with: {trainable_params} parameters\")\n",
    "\n",
    "C.set_lora_mode(False)\n",
    "trainable_params = sum(p.numel() for p in C.parameters() if p.requires_grad)\n",
    "print(f\"Training with: {trainable_params} parameters\")\n",
    "\n",
    "\n",
    "\n",
    "C.set_lora_mode(True)\n",
    "trainable_params = sum(p.numel() for p in C.parameters() if p.requires_grad)\n",
    "print(f\"Finetuning with: {trainable_params} parameters\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61_u4dyVAiHe"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFk3DN4c_8-Z"
   },
   "outputs": [],
   "source": [
    "img_size = (128,128)\n",
    "max_text_seq_length = 128\n",
    "\n",
    "dataset = load_dataset(\"fashion_mnist\")\n",
    "dataset.set_format(type='torch')\n",
    "num_train_samples = len(dataset['train'])\n",
    "split_10 = int(0.1 * num_train_samples)\n",
    "split_90 = num_train_samples - split_10\n",
    "mnist_train_dataset, mnist_val_dataset = random_split(dataset['train'], [split_90, split_10], generator=torch.Generator().manual_seed(42))\n",
    "mnist_test_dataset = dataset[\"test\"]\n",
    "\n",
    "\n",
    "class FashionMNIST(Dataset):\n",
    "    def __init__(self, split=\"train\"):\n",
    "        if split==\"train\":\n",
    "            self.dataset = mnist_train_dataset\n",
    "            self.split = \"train\"\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize(img_size),\n",
    "                #T.ToTensor(),\n",
    "                T.ConvertImageDtype(torch.float32)  # Convert the image to float32 tensor\n",
    "            ])\n",
    "        elif split==\"val\":\n",
    "            self.dataset = mnist_val_dataset\n",
    "            self.split = \"val\"\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize(img_size),\n",
    "                #T.ToTensor(),\n",
    "                T.ConvertImageDtype(torch.float32)  # Convert the image to float32 tensor\n",
    "            ])\n",
    "        else:\n",
    "            self.dataset = mnist_test_dataset\n",
    "            self.split = \"test\"\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize(img_size),\n",
    "                #T.ToTensor(),\n",
    "                T.ConvertImageDtype(torch.float32)  # Convert the image to float32 tensor\n",
    "            ])\n",
    "\n",
    "        #print(type(self.dataset))\n",
    "        #print(dir(self.dataset))\n",
    "\n",
    "\n",
    "\n",
    "        self.captions = {\n",
    "            0: \"t-shirt/top\",\n",
    "            1: \"trousers\",\n",
    "            2: \"pullover\",\n",
    "            3: \"dress\",\n",
    "            4: \"coat\",\n",
    "            5: \"sandal\",\n",
    "            6: \"shirt\",\n",
    "            7: \"sneaker\",\n",
    "            8: \"bag\",\n",
    "            9: \"ankle boot\"\n",
    "        }\n",
    "        self.captions = {k: f\"an image of {v}\" for k, v in self.captions.items()}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        img = self.dataset[i][\"image\"]\n",
    "        img = self.transform(img)\n",
    "\n",
    "        cap, mask = tokenizer(self.captions[self.dataset[i][\"label\"].item()], max_seq_length = max_text_seq_length)\n",
    "\n",
    "        mask = mask.repeat(len(mask),1)\n",
    "\n",
    "        return {\"image\": img, \"caption\": cap, \"mask\": mask}\n",
    "#ds = FashionMNIST(split=\"train\")\n",
    "#for i in ds:\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3um_vL-3fl8x"
   },
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "              T.Resize(img_size),  # Resize to a fixed size (e.g., for models like ResNet)\n",
    "              T.Grayscale(num_output_channels=1),  # Convert to grayscale with 1 output channel\n",
    "              T.ToTensor(),\n",
    "            ])\n",
    "\n",
    "dataset = Caltech101(root=\"./\", target_type='category', transform=transform, download=True)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "cal_train_dataset, cal_val_dataset, cal_test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "class Caltech101_Wrapper(Dataset):\n",
    "    def __init__(self, split=\"train\"):\n",
    "\n",
    "        # Define transformations (you can adjust these based on model requirements\n",
    "\n",
    "        if split == \"train\":\n",
    "          self.dataset = cal_train_dataset\n",
    "        elif split == \"val\":\n",
    "          self.dataset = cal_val_dataset\n",
    "        else:\n",
    "          self.dataset = cal_test_dataset\n",
    "\n",
    "\n",
    "\n",
    "        # Load the Caltech101 dataset\n",
    "\n",
    "        # Captions for the different object categories\n",
    "        self.captions = {\n",
    "          0: \"accordion\",\n",
    "          1: \"airplane\",\n",
    "          2: \"anchor\",\n",
    "          3: \"ant\",\n",
    "          4: \"from Google backgrounds\",\n",
    "          5: \"barrel\",\n",
    "          6: \"bass fish\",\n",
    "          7: \"beaver\",\n",
    "          8: \"binoculars\",\n",
    "          9: \"bonsai tree\",\n",
    "          10: \"brain\",\n",
    "          11: \"brontosaurus\",\n",
    "          12: \"Buddha statue\",\n",
    "          13: \"butterfly\",\n",
    "          14: \"camera\",\n",
    "          15: \"cannon\",\n",
    "          16: \"car viewed from the side\",\n",
    "          17: \"ceiling fan\",\n",
    "          18: \"cellphone\",\n",
    "          19: \"chair\",\n",
    "          20: \"chandelier\",\n",
    "          21: \"cougar's body\",\n",
    "          22: \"cougar's face\",\n",
    "          23: \"crab\",\n",
    "          24: \"crayfish\",\n",
    "          25: \"crocodile\",\n",
    "          26: \"crocodile's head\",\n",
    "          27: \"cup\",\n",
    "          28: \"dalmatian dog\",\n",
    "          29: \"dollar bill\",\n",
    "          30: \"dolphin\",\n",
    "          31: \"dragonfly\",\n",
    "          32: \"electric guitar\",\n",
    "          33: \"elephant\",\n",
    "          34: \"emu\",\n",
    "          35: \"euphonium instrument\",\n",
    "          36: \"ewer\",\n",
    "          37: \"face\",\n",
    "          38: \"face in an easy-to-recognize pose\",\n",
    "          39: \"ferry\",\n",
    "          40: \"flamingo\",\n",
    "          41: \"flamingo's head\",\n",
    "          42: \"character Garfield\",\n",
    "          43: \"gerenuk\",\n",
    "          44: \"gramophone\",\n",
    "          45: \"grand piano\",\n",
    "          46: \"hawksbill turtle\",\n",
    "          47: \"headphones\",\n",
    "          48: \"hedgehog\",\n",
    "          49: \"helicopter\",\n",
    "          50: \"ibis bird\",\n",
    "          51: \"inline skate\",\n",
    "          52: \"Joshua tree\",\n",
    "          53: \"kangaroo\",\n",
    "          54: \"ketch sailboat\",\n",
    "          55: \"lamp\",\n",
    "          56: \"laptop\",\n",
    "          57: \"leopard\",\n",
    "          58: \"llama\",\n",
    "          59: \"lobster\",\n",
    "          60: \"lotus flower\",\n",
    "          61: \"mandolin\",\n",
    "          62: \"mayfly\",\n",
    "          63: \"menorah\",\n",
    "          64: \"metronome\",\n",
    "          65: \"minaret\",\n",
    "          66: \"motorbike\",\n",
    "          67: \"nautilus shell\",\n",
    "          68: \"octopus\",\n",
    "          69: \"okapi\",\n",
    "          70: \"pagoda\",\n",
    "          71: \"panda bear\",\n",
    "          72: \"pigeon\",\n",
    "          73: \"pizza\",\n",
    "          74: \"platypus\",\n",
    "          75: \"pyramid\",\n",
    "          76: \"revolver gun\",\n",
    "          77: \"rhinoceros\",\n",
    "          78: \"rooster\",\n",
    "          79: \"saxophone\",\n",
    "          80: \"schooner sailboat\",\n",
    "          81: \"pair of scissors\",\n",
    "          82: \"scorpion\",\n",
    "          83: \"sea horse\",\n",
    "          84: \"soccer ball\",\n",
    "          85: \"character Snoopy\",\n",
    "          86: \"starfish\",\n",
    "          87: \"stapler\",\n",
    "          88: \"stegosaurus\",\n",
    "          89: \"stop sign\",\n",
    "          90: \"strawberry\",\n",
    "          91: \"sunflower\",\n",
    "          92: \"tick insect\",\n",
    "          93: \"trilobite fossil\",\n",
    "          94: \"umbrella\",\n",
    "          95: \"watch\",\n",
    "          96: \"water lily\",\n",
    "          97: \"wheelchair\",\n",
    "          98: \"wild cat\",\n",
    "          99: \"Windsor chair\",\n",
    "          100: \"wrench tool\",\n",
    "          101: \"yin-yang symbol\"\n",
    "          }\n",
    "        self.captions = {k: f\"an image of {v}\" for k, v in self.captions.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Get the image and label from the dataset\n",
    "        #print(self.dataset[i])\n",
    "        #print(type(self.dataset[i]))\n",
    "        #print(dir(self.dataset[i]))\n",
    "        img = self.dataset[i][0]\n",
    "        #print(f\"img: {img.shape}\")\n",
    "        label = self.dataset[i][1]\n",
    "        #print(f\"label: {label}\")\n",
    "\n",
    "        # Apply the transformation to the image\n",
    "        #img = self.transform(img)\n",
    "\n",
    "        # Generate the caption based on the label\n",
    "        caption = self.captions.get(label, \"An image of an object\")  # Default caption if not found\n",
    "        #print(f\"caption: {caption}: {len(caption)}\")\n",
    "\n",
    "        # Tokenize the caption and create the mask (use your tokenizer accordingly)\n",
    "        cap, mask = tokenizer(caption, max_seq_length = max_text_seq_length)\n",
    "        #print(f\"tokens: {cap} {cap.shape}\")\n",
    "        mask = mask.repeat(len(mask), 1)\n",
    "        #print(f\"mask: {mask} {mask.shape}\")\n",
    "        #print()\n",
    "\n",
    "        # Return the processed image, caption, and mask\n",
    "        return {\"image\": img, \"caption\": cap, \"mask\": mask}\n",
    "\n",
    "#Caltech101_Wrapper(split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9XApEuyAutN"
   },
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzWJYpjf6_Q1"
   },
   "outputs": [],
   "source": [
    "def epoch_func(model, data_loader, optimizer = None):\n",
    "    start_time = time.time()\n",
    "    loss_acc = 0\n",
    "    N = 0\n",
    "    for i, data in enumerate(data_loader):\n",
    "        img, cap, mask = data[\"image\"].to(device), data[\"caption\"].to(device), data[\"mask\"].to(device)\n",
    "        loss = model(img,cap,mask)\n",
    "        loss_acc += loss\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        N += len(data[\"image\"])\n",
    "        print(f\"\\rBatch:{(i+1)}/{len(data_loader):.2f}, Avg Loss: {loss_acc/N:.5f}, {time.time()-start_time:.2f}s\", end='', flush=True)\n",
    "        del loss, img, cap, mask\n",
    "\n",
    "    print()\n",
    "    return (loss_acc/N).detach().cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1P9oGT52ABYi"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, epochs, lr, save_dir, val_loader):\n",
    "    os.makedirs(save_dir,  exist_ok=True)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    with torch.no_grad():\n",
    "        initial_training_loss = epoch_func(model, train_loader, optimizer=None)\n",
    "        training_losses = [initial_training_loss]\n",
    "\n",
    "        initial_val_loss = epoch_func(model, val_loader, optimizer=None)\n",
    "        validation_losses = [initial_val_loss]\n",
    "    torch.cuda.empty_cache()\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    num_optimized_params = sum(p.numel() for group in optimizer.param_groups for p in group['params'] if p.requires_grad)\n",
    "    assert trainable_params == num_optimized_params\n",
    "    print(f\"Starting training {save_dir} with: {trainable_params} parameters\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "        avg_training_loss = epoch_func(model, train_loader, optimizer=optimizer)\n",
    "        training_losses.append(avg_training_loss)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            avg_validation_loss = epoch_func(model, val_loader, optimizer=None)\n",
    "            validation_losses.append(avg_validation_loss)\n",
    "\n",
    "        if avg_validation_loss < best_loss:\n",
    "            best_loss = avg_validation_loss\n",
    "            torch.save(model.state_dict(), f\"{save_dir}clip.pt\")\n",
    "            print(\"Model Saved.\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    plt.plot(training_losses, label = \"training\")\n",
    "    if val_loader is not None:\n",
    "        plt.plot(validation_losses, label = \"validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training Loss {save_dir}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{save_dir}Training.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkS-qo1R6_Q1"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_set, data_loader):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Getting dataset captions to compare images to\n",
    "    text = torch.stack([tokenizer(x, max_seq_length)[0] for x in data_set.captions.values()]).to(device)\n",
    "    #print(f\"{text.shape=}\")\n",
    "    mask = torch.stack([tokenizer(x, max_seq_length)[1] for x in data_set.captions.values()])\n",
    "    #print(f\"{mask.shape=}\")\n",
    "\n",
    "    mask = mask.repeat(1,len(mask[0])).reshape(len(mask),len(mask[0]),len(mask[0])).to(device)\n",
    "    #print(f\"{mask.shape=}\")\n",
    "\n",
    "    text_features = model.text_encoder(text, mask=mask)\n",
    "    #print(f\"{text_features.shape=}\")\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    #print(f\"{text_features.shape=}\")\n",
    "\n",
    "    correct, total = 0,0\n",
    "    #print(\"\\n\\n\")\n",
    "    for i, data in enumerate(data_loader):\n",
    "        images, labels = data[\"image\"].to(device), data[\"caption\"].to(device)\n",
    "        #print(f\"{images.shape=}\")\n",
    "        #print(f\"{labels}{labels.shape}\")\n",
    "\n",
    "        image_features = model.image_encoder(images)\n",
    "        #print(f\"{image_features.shape=}\")\n",
    "\n",
    "\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        #print(f\"{image_features.shape=}\")\n",
    "\n",
    "        similarity = (100.0 * (image_features @ text_features.T)).softmax(dim=-1)\n",
    "        #print(f\"{similarity.shape=}\")\n",
    "\n",
    "        _, indices = torch.max(similarity,1)\n",
    "        #print(f\"{indices.shape=}\")\n",
    "        #print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "        pred = torch.stack([tokenizer(data_set.captions[int(i)], max_seq_length=max_seq_length)[0] for i in indices]).to(device)\n",
    "        #print(f\"{pred.shape=}\")\n",
    "        #print(f\"{pred==labels.shape}\")\n",
    "        #print(f\"{torch.sum((pred==labels),dim=1)=}\")\n",
    "        #print(f\"{torch.sum((pred==labels),dim=1)//len(pred[0])=}\")\n",
    "        #print(f\"{sum(torch.sum((pred==labels),dim=1)//len(pred[0]))=}\")\n",
    "        correct += int(sum(torch.sum((pred==labels),dim=1)//len(pred[0])))\n",
    "        #print(f\"{correct}\")\n",
    "        total += len(labels)\n",
    "\n",
    "        print(f\"\\rBatch:{(i+1)}/{len(data_loader)} Acc:{100 * (correct / total):.2f}%, {time.time()-start_time:0.2f}s\", end='', flush=True)\n",
    "    print()\n",
    "\n",
    "    return correct/total\n",
    "#model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers, lora_rank).to(device)\n",
    "#get_accuracy(model, cal_test_set, cal_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYGpMX_PNXi6"
   },
   "outputs": [],
   "source": [
    "emb_dim = 32\n",
    "\n",
    "encoder_heads=4\n",
    "encoder_width=32\n",
    "\n",
    "patch_size = (8,8)\n",
    "n_channels = 1\n",
    "vit_width = encoder_width\n",
    "vit_heads = encoder_heads\n",
    "\n",
    "max_seq_length = max_text_seq_length\n",
    "vocab_size = 256\n",
    "text_width = encoder_width\n",
    "text_heads = encoder_heads\n",
    "\n",
    "training_epochs=32\n",
    "lora_epochs = 8\n",
    "\n",
    "training_batch_size = 128\n",
    "lora_batch_size = 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_hOwteR_96v"
   },
   "outputs": [],
   "source": [
    "mnist_train_set = FashionMNIST(split = \"train\")\n",
    "mnist_val_set = FashionMNIST(split = \"val\")\n",
    "mnist_test_set = FashionMNIST(split = \"test\")\n",
    "\n",
    "mnist_train_loader = DataLoader(mnist_train_set, shuffle=True, batch_size=lora_batch_size, num_workers=4)\n",
    "mnist_val_loader = DataLoader(mnist_val_set, shuffle=True, batch_size=lora_batch_size, num_workers=4)\n",
    "mnist_test_loader = DataLoader(mnist_test_set, shuffle=False, batch_size=lora_batch_size, num_workers=4)\n",
    "\n",
    "cal_train_set = Caltech101_Wrapper(split = \"train\")\n",
    "cal_val_set = Caltech101_Wrapper(split = \"val\")\n",
    "cal_test_set = Caltech101_Wrapper(split = \"test\")\n",
    "\n",
    "cal_train_loader = DataLoader(cal_train_set, shuffle=True, batch_size=training_batch_size, num_workers = 4)\n",
    "cal_val_loader = DataLoader(cal_val_set, shuffle=True, batch_size=training_batch_size, num_workers = 4)\n",
    "cal_test_loader = DataLoader(cal_test_set, shuffle=True, batch_size=training_batch_size, num_workers = 4)\n",
    "\n",
    "def plot_batch(loader):\n",
    "  batch_size = loader.batch_size\n",
    "  fig_side_length = int(np.ceil(np.sqrt(batch_size)))\n",
    "  #print(f\"fig_side_length: {fig_side_length}\")\n",
    "  fig, axes = plt.subplots(fig_side_length,fig_side_length)\n",
    "  for batch in loader:\n",
    "    for i, img in enumerate(batch[\"image\"], 0):\n",
    "      caption = batch[\"caption\"][i]\n",
    "      #print(f\"caption.shape: {caption.shape}\")64\n",
    "      mask = batch[\"mask\"][i]\n",
    "      #print(f\"mask.shape: {mask.shape}\")\n",
    "      img = img.permute(1, 2, 0).numpy()\n",
    "      row = i // fig_side_length\n",
    "      col = i % fig_side_length\n",
    "      #print(f\"i:{i}, row:{row}, col:{col}\")\n",
    "      axes[row, col].imshow(img,cmap=\"gray\")\n",
    "      axes[row, col].axis(\"off\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "#plot_batch(mnist_train_loader)\n",
    "#plot_batch(mnist_val_loader)\n",
    "#plot_batch(mnist_test_loader)\n",
    "#plot_batch(cal_train_loader)\n",
    "#plot_batch(cal_val_loader)\n",
    "#plot_batch(cal_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T13MPedI6_Q1"
   },
   "outputs": [],
   "source": [
    "def run_expirment(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads,\n",
    "                vocab_size, text_width, max_seq_length, text_heads, text_layers, lora_rank, lr, lora_lr, save_dir):\n",
    "\n",
    "\n",
    "\n",
    "    model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers, lora_rank).to(device)\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"When training: {trainable_params} parameters\")\n",
    "\n",
    "    model.set_lora_mode(True)\n",
    "    ftable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"When finetuning: {ftable_params} parameters\")\n",
    "    model.set_lora_mode(False)\n",
    "\n",
    "    train_model(model, cal_train_loader, training_epochs, lr, f\"{save_dir}CAL/\", val_loader=cal_val_loader)\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"{save_dir}CAL/clip.pt\", map_location=device, weights_only = True))\n",
    "    with torch.no_grad():\n",
    "        cal_accuracy = get_accuracy(model, cal_test_set, cal_test_loader)\n",
    "\n",
    "\n",
    "    print(\"\\n\\n\\n\")\n",
    "\n",
    "    model.set_lora_mode(True)\n",
    "\n",
    "    train_model(model, mnist_train_loader, lora_epochs, lora_lr, f\"{save_dir}MNIST/\", val_loader=mnist_val_loader)\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"{save_dir}MNIST/clip.pt\", map_location=device, weights_only = True))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ft_accuracy = get_accuracy(model, mnist_test_set, mnist_test_loader)\n",
    "    model.set_lora_mode(False)\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model.set_lora_mode(True)\n",
    "    ftable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    with open(f\"{save_dir}metrics.txt\", \"w\") as file:\n",
    "        file.write(f\"trainable params: {trainable_params}\\n\")\n",
    "        file.write(f\"Caltech Accuracy: {cal_accuracy}\\n\")\n",
    "        file.write(f\"finetuneable params: {ftable_params}\\n\")\n",
    "        file.write(f\"MNIST accuracy: {ft_accuracy}\\n\")\n",
    "    return cal_accuracy, ft_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3i4pmkIE6_Q1"
   },
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "best_cal_accuracy =-1\n",
    "best_cal_params = [0, 0]\n",
    "cal_performance_history = []\n",
    "best_ft_accuracy = -1\n",
    "best_ft_params = [0, 0]\n",
    "ft_performance_history = []\n",
    "\n",
    "\n",
    "layers_to_try = [4]#[1, 4, 8, 16, 32]\n",
    "ranks_to_try = [1, 4, 16]\n",
    "learning_rates_to_try =[1e-3]#, 1e-4, 1e-5]\n",
    "\n",
    "parent_dir = f\"/content/drive/MyDrive/last_run/\"\n",
    "os.makedirs(parent_dir, exist_ok=True)\n",
    "\n",
    "epochs_dir = f\"{parent_dir}{training_epochs=}-{lora_epochs=}/\"\n",
    "os.makedirs(epochs_dir, exist_ok=True)\n",
    "\n",
    "for lr in learning_rates_to_try:\n",
    "    lora_lr = lr\n",
    "    lr_dir = f\"{epochs_dir}{lr=}-{training_batch_size=}-{lora_batch_size=}/\"\n",
    "    os.makedirs(lr_dir, exist_ok=True)\n",
    "    for layers in layers_to_try:\n",
    "        layer_dir = f\"{lr_dir}{layers=}/\"\n",
    "        for lora_rank in ranks_to_try:\n",
    "            model_dir = f\"{layer_dir}w:{encoder_width}--h:{encoder_heads}-ed:{emb_dim}-rank:{lora_rank}/\"\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            cal_accuracy, ft_accuracy = run_expirment(emb_dim, vit_width, img_size, patch_size, n_channels, layers, vit_heads,\n",
    "                                                      vocab_size, text_width, max_seq_length, text_heads, layers, lora_rank, lr, lora_lr,\n",
    "                                                       model_dir)\n",
    "            #cal_accuracy = np.random.rand()\n",
    "            #ft_accuracy = np.random.rand()\n",
    "\n",
    "            if cal_accuracy>best_cal_accuracy:\n",
    "                best_cal_accuracy = cal_accuracy\n",
    "                best_cal_params =[lr, layers, lora_rank]\n",
    "                print(f\"New best cal model:\\n{best_cal_accuracy=}\\n{best_cal_params=}\")\n",
    "            if ft_accuracy>best_ft_accuracy:\n",
    "                best_ft_accuracy = ft_accuracy\n",
    "                best_ft_params =[lr, layers, lora_rank]\n",
    "                print(f\"New best ft model:\\n{best_ft_accuracy=}\\n{best_ft_params=}\")\n",
    "            cal_performance_history.append((cal_accuracy, [lr, layers, lora_rank]))\n",
    "            ft_performance_history.append((ft_accuracy, [lr, layers, lora_rank]))\n",
    "        print(\"\\n\\n\\n\")\n",
    "\n",
    "print(f\"finished experiment in {time.time()-start_time:.2f} seconds\")\n",
    "with open(f\"{epochs_dir}performances.txt\", \"w\") as file:\n",
    "        file.write(f\"{best_cal_accuracy=}\\n\")\n",
    "        file.write(f\"{best_cal_params=}\\n\")\n",
    "\n",
    "        file.write(f\"{best_ft_accuracy=}\\n\")\n",
    "        file.write(f\"{best_ft_params=}\\n\")\n",
    "\n",
    "        file.write(f\"{cal_performance_history=}\\n\")\n",
    "        file.write(f\"{ft_performance_history=}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Mz0Cu1N6_Q2"
   },
   "outputs": [],
   "source": [
    "for history, name in [(cal_performance_history, \"cal\"), (ft_performance_history, \"MNIST\")]:\n",
    "    fig_width = len(history)  # Adjust multiplier as needed for spacing\n",
    "    n_axes = len(learning_rates_to_try)\n",
    "    points_per_ax = len(layers_to_try) * len(ranks_to_try)\n",
    "    fig_height = n_axes*6  # Fixed height\n",
    "\n",
    "    # Create the bar chart with dynamic figure size\n",
    "    fig, axes = plt.subplots(nrows=n_axes, ncols=1, figsize=(fig_width, fig_height))\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    values = [entry[0] for entry in history]\n",
    "    labels = [f\"({entry[1][1]}, {entry[1][2]})\" for entry in history]\n",
    "    for i in range(n_axes):\n",
    "        axes[i].set_xlabel(f\"learning_rate = {learning_rates_to_try[i]}, (layers, rank)\")\n",
    "        axes[i].set_ylabel(\"accuracy\")\n",
    "        #axes.set_xlabel(f\"learning_rate = {learning_rates_to_try[i]}, (layers, rank)\")\n",
    "        #axes.set_ylabel(\"accuracy\")\n",
    "\n",
    "    colors = [\"red\",\"yellow\",\"green\",\"orange\", \"blue\", \"purple\"]\n",
    "    colors = [colors[(i//len(ranks_to_try))%len(colors)] for i in range(len(values))]\n",
    "    for i in range(n_axes):\n",
    "             starting_index = i * points_per_ax\n",
    "             ending_index = (i+1) * points_per_ax\n",
    "             axes[i].bar(labels[starting_index:ending_index], values[starting_index:ending_index], color=colors)\n",
    "             #axes.bar(labels[starting_index:ending_index], values[starting_index:ending_index], color=colors)\n",
    "\n",
    "\n",
    "\n",
    "    fig.suptitle(f'{epochs_dir}{name}_performacne', y=1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.savefig(f\"{epochs_dir}{name}_history.png\")\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "872DyksS6_Q2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pwt8Po7V6_Q2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
